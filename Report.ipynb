{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I provided a description of my implementation for the third project -  **collaboration and competition** of the [Deep Reinforcement Learning Nanodegree](https//www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  Before running the code cell below, make sure the location of the Unity environment that you downloaded is as described in `Readme.md` file - `env/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"env/Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print(\"Default brain is:\", brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('\\nThere are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('\\nThe state for the first agent looks like:', states[0])\n",
    "print('\\nThe state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the DDPG Agent(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Smart Agent that is going to be learned to control the double-jointed arm is coded in the [ddpg_agent.py](./ddpg_agent.py). \n",
    "\n",
    "The agent is constructed with DDPG learning algorithm. The agent can be initialized for learning or for play only based on a saved checkpoint (see below).\n",
    "\n",
    "The DDPG (Deep Deterministic Poicy Gradient) actor-critic approach is recommended for this problem, main reason being the fact that agent must learn a continous spectrum of actions. \n",
    "\n",
    "The actor and critic NN models are coded in the [ddpg_model.py](./ddpg_model.py). The two models have two hidden layers each. At input the critic NN it gets a tensor having a number of elements equal with the number of state elements and in second layer also the action is introduced as input, while is producing at the output the estimated Q value the given state-action pair. The actor NN gets a tensor having a number of elements equal with number of states elements and is producing at the output the action value.\n",
    "\n",
    "This type of algorithm is an of-policy one and combined with the fact that function approximation is used plus bootstraping this leads to potentially stability issues. The trick to stabilize the learning is use of replay buffer similar with the DQN algorithm.\n",
    "\n",
    "The exploration is ensured by adding a noise to the actor NN returned action value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiate the learning Agent defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Agent and load from the filesystem the NN weights from checkpoints (if exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Agent with DDPG Algorithm\n",
    "\n",
    "Run the code cell below to train the agent (from scratch or continue training the models loaded from the files). The NN parameters are saved in files every 100 episodes so that traiing can be resumed later. Training stops when the average score over past 100 episodes goes beyond 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(environment, agents, weights_actors, weights_critics, n_episodes=2000):\n",
    "    brain_name = environment.brain_names[0]\n",
    "    environment_info = environment.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    agents_size = len(agents)  # number of agents\n",
    "    states_size = agents_size * environment_info.vector_observations.shape[1]  # size of the state space shared by the agents\n",
    "\n",
    "    scores = []  # scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        environment_info = environment.reset(train_mode=True)[brain_name]\n",
    "        states = environment_info.vector_observations.reshape((1, states_size))\n",
    "\n",
    "        # Reset the agents\n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "\n",
    "        scores_agents = np.zeros(agents_size)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Perform actions in the environment\n",
    "            actions = [agent.act(states, True) for agent in agents]  # execute actions with added noise\n",
    "            actions = np.hstack(tuple(actions))  # stack the actions performed by the agents\n",
    "\n",
    "            environment_info = environment.step(actions)[brain_name]  # send both agents' actions together to the environment\n",
    "            next_states = environment_info.vector_observations.reshape((1, states_size))\n",
    "\n",
    "            rewards = environment_info.rewards  # get reward\n",
    "            dones = environment_info.local_done  # verify if episode finished\n",
    "\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.step(states, actions, rewards[i], next_states, dones[i], i)  # agent i learns\n",
    "\n",
    "            scores_agents += rewards  # update the score for each agent\n",
    "            states = next_states  # roll over states to next time step\n",
    "\n",
    "            if np.any(dones):\n",
    "                break\n",
    "            \n",
    "        scores_window.append(np.max(scores_agents))\n",
    "        scores.append(np.max(scores_agents))\n",
    "        \n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}'.format(i_episode, np.mean(scores_window), np.max(scores_agents), end=\"\"))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "            for agent, weights_actor, weights_critic in zip(agents, weights_actors, weights_critics):\n",
    "                torch.save(agent.actor_local.state_dict(), weights_actor)\n",
    "                torch.save(agent.critic_local.state_dict(), weights_critic)\n",
    "        \n",
    "        if np.mean(scores_window)>=0.5: # stop learning if the average score for the last 100 episodes is greater than 0.5\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.4f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agents = [\n",
    "        Agent(state_size=state_size, action_size=action_size, random_seed=0),\n",
    "        Agent(state_size=state_size, action_size=action_size, random_seed=0)\n",
    "]\n",
    "\n",
    "weights_actors = [\n",
    "        \"actor1.pth\",\n",
    "        \"actor2.pth\"\n",
    "]\n",
    "\n",
    "    # Retrieve weights for the critics\n",
    "weights_critics = [\n",
    "        \"critic1.pth\",\n",
    "        \"critic2.pth\"    \n",
    "]\n",
    "\n",
    "scores_list = ddpg(environment=env, agents=agents, weights_actors=weights_actors, weights_critics=weights_critics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_list)+1), scores_list)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "\n",
    "window_mean = pd.Series(scores).rolling(100).mean()\n",
    "plt.plot(window_mean, linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several improvements that can be made:\n",
    "   - tune the hyper-parameters to train even faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
